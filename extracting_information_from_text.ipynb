{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Chunking:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phrase chunking and tag patterns using regex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),\n",
    "    (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\" # tag pattern\n",
    "\n",
    "cp = nltk.RegexpParser(grammar) # chunk parser\n",
    "result = cp.parse(sentence)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tag pattern combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "    NP: {<DT|PP\\$>?<JJ>*<NN.*>} # determiner/ possesive, adjective and noun\n",
    "        {<NNP>+}                # proper nouns\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentence = [(\"Rapunzel\", \"NNP\"), (\"let\", \"VBD\"), (\"down\", \"RP\"),\n",
    "                 (\"her\", \"PP$\"), (\"long\", \"JJ\"), (\"golden\", \"JJ\"), (\"hair\", \"NN\")]\n",
    "result = cp.parse(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Overlapping matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP money/NN market/NN) fund/NN)\n"
     ]
    }
   ],
   "source": [
    "nouns = [(\"money\", \"NN\"), (\"market\", \"NN\"), (\"fund\", \"NN\")]\n",
    "grammar = \"NP: {<NN>{2}}  # Chunk two consecutive nouns\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "print(cp.parse(nouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring text corpora:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search text corpora using a tag pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CHUNK combined/VBN to/TO achieve/VB)\n",
      "(CHUNK continue/VB to/TO place/VB)\n",
      "(CHUNK serve/VB to/TO protect/VB)\n",
      "(CHUNK wanted/VBD to/TO wait/VB)\n"
     ]
    }
   ],
   "source": [
    "grammar = 'CHUNK: {<V.*> <TO> <V.*>}'\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "brown = nltk.corpus.brown\n",
    "\n",
    "for x in brown.tagged_sents()[:50]:\n",
    "    tree = cp.parse(x)\n",
    "    for y in tree.subtrees():\n",
    "        if y.label() == 'CHUNK': print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NOUNS Court/NN-TL Judge/NN-TL Durwood/NP Pye/NP)\n",
      "(NOUNS Mayor-nominate/NN-TL Ivan/NP Allen/NP Jr./NP)\n",
      "(NOUNS Georgia's/NP$ automobile/NN title/NN law/NN)\n",
      "(NOUNS State/NN-TL Welfare/NN-TL Department's/NN$-TL handling/NN)\n",
      "(NOUNS Fulton/NP-TL Tax/NN-TL Commissioner's/NN$-TL Office/NN-TL)\n",
      "(NOUNS Mayor/NN-TL William/NP B./NP Hartsfield/NP)\n",
      "(NOUNS Mrs./NP J./NP M./NP Cheshire/NP)\n",
      "(NOUNS E./NP Pelham/NP Rd./NN-TL Aj/NN)\n",
      "(NOUNS\n",
      "  State/NN-TL\n",
      "  Party/NN-TL\n",
      "  Chairman/NN-TL\n",
      "  James/NP\n",
      "  W./NP\n",
      "  Dorsey/NP)\n",
      "(NOUNS Texas/NP Sen./NN-TL John/NP Tower/NP)\n"
     ]
    }
   ],
   "source": [
    "def find_chunks(grammar, tagged_sents):\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    for x in tagged_sents:\n",
    "        tree = cp.parse(x)\n",
    "        for y in tree.subtrees():\n",
    "            if y.label() == grammar.split(':')[0]: print(y)\n",
    "                \n",
    "tagged_sents = brown.tagged_sents()[:50]\n",
    "grammar = \"NOUNS: {<N.*>{4,}}\"\n",
    "\n",
    "find_chunks(grammar, tagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chinking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"\"\"\n",
    "    NP:\n",
    "        {<.*>+}       # Chunk everything\n",
    "        }<VBD|IN>+{   # Chink sequences of VBD and IN\n",
    "\"\"\"\n",
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),\n",
    "       (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "print(cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Developing and evaluating chunkers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "he PRP B-NP\n",
    "accepted VBD B-VP\n",
    "the DT B-NP\n",
    "position NN I-NP\n",
    "of IN B-PP\n",
    "vice NN B-NP\n",
    "chairman NN I-NP\n",
    "of IN B-PP\n",
    "Carlyle NNP B-NP\n",
    "Group NNP I-NP\n",
    ", , O\n",
    "a DT B-NP\n",
    "merchant NN I-NP\n",
    "banking NN I-NP\n",
    "concern NN I-NP\n",
    ". . O\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.chunk.conllstr2tree(text, chunk_types=['NP', 'PP']).draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple evaluation and baselines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  43.4%%\n",
      "    Precision:      0.0%%\n",
      "    Recall:         0.0%%\n",
      "    F-Measure:      0.0%%\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "cp = nltk.RegexpParser(\"\") # searching for 'O' tag (i.e. not in chunk)\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "print(cp.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  87.7%%\n",
      "    Precision:     70.6%%\n",
      "    Recall:        67.8%%\n",
      "    F-Measure:     69.2%%\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"NP: {<[CDJNP].*>+}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "print(cp.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t, c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.UnigramTagger(train_data)\n",
    "        \n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for word, pos in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for pos, chunktag in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.9%%\n",
      "    Precision:     79.9%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     83.2%%\n"
     ]
    }
   ],
   "source": [
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
    "unigram_chuncker = UnigramChunker(train_sents)\n",
    "print(unigram_chuncker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT', 'O'),\n",
       " ('Fulton', 'NP-TL', 'O'),\n",
       " ('County', 'NN-TL', 'O'),\n",
       " ('Grand', 'JJ-TL', 'O'),\n",
       " ('Jury', 'NN-TL', 'O'),\n",
       " ('said', 'VBD', 'O'),\n",
       " ('Friday', 'NR', 'O'),\n",
       " ('an', 'AT', 'O'),\n",
       " ('investigation', 'NN', 'B-NP'),\n",
       " ('of', 'IN', 'O'),\n",
       " (\"Atlanta's\", 'NP$', 'O'),\n",
       " ('recent', 'JJ', 'B-NP'),\n",
       " ('primary', 'NN', 'I-NP'),\n",
       " ('election', 'NN', 'I-NP'),\n",
       " ('produced', 'VBD', 'O'),\n",
       " ('``', '``', 'O'),\n",
       " ('no', 'AT', 'O'),\n",
       " ('evidence', 'NN', 'B-NP'),\n",
       " (\"''\", \"''\", 'O'),\n",
       " ('that', 'CS', 'O'),\n",
       " ('any', 'DTI', 'O'),\n",
       " ('irregularities', 'NNS', 'B-NP'),\n",
       " ('took', 'VBD', 'O'),\n",
       " ('place', 'NN', 'B-NP'),\n",
       " ('.', '.', 'O')]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = nltk.corpus.brown.tagged_sents()[0]\n",
    "nltk.chunk.tree2conlltags(unigram_chuncker.parse(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
