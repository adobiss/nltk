{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Supervised classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gender identification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last_letter': 'k'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def gender_features(name):\n",
    "    return {'last_letter': name[-1]\n",
    "            #, 'first_letter': word[0]\n",
    "            #, 'name_length': len(word)\n",
    "           } # builds a feature set: human-readable feature name and value pair\n",
    "\n",
    "gender_features('Shrek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features2(name):\n",
    "    features = {}\n",
    "    features['first_letter'] = name[0].lower()\n",
    "    features['last_letter'] = name[-1].lower()\n",
    "    for x in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features['count({})'.format(x)] = name.lower().count(x)\n",
    "        features['has({})'.format(x)] = (x in name.lower())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Kasey', 'female'),\n",
       " ('Hollis', 'male'),\n",
       " ('Berk', 'male'),\n",
       " ('Hatti', 'female'),\n",
       " ('Aime', 'female')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import names\n",
    "\n",
    "labeled_names = ([(x, 'male') for x in names.words('male.txt')] +\n",
    "                [(x, 'female') for x in names.words('female.txt')])\n",
    "\n",
    "import random\n",
    "random.shuffle(labeled_names)\n",
    "labeled_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'female': 5001, 'male': 2943})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist([y for x, y in labeled_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(gender_features2(x), y) for x, y in labeled_names]\n",
    "\n",
    "#train_set, test_set = featuresets[500:], featuresets[:500] # When feature set is relatively small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When feature set is large\n",
    "\n",
    "from nltk.classify import apply_features\n",
    "\n",
    "train_set = apply_features(gender_features2, labeled_names[500:])\n",
    "test_set = apply_features(gender_features2, labeled_names[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7944, 7444)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(featuresets), len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trinity is a female\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "name = 'Trinity'\n",
    "print('{} is a '.format(name) + classifier.classify(gender_features2(name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find most the most effective features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     37.0 : 1.0\n",
      "             last_letter = 'k'              male : female =     31.9 : 1.0\n",
      "             last_letter = 'f'              male : female =     17.1 : 1.0\n",
      "             last_letter = 'v'              male : female =     11.1 : 1.0\n",
      "             last_letter = 'd'              male : female =      9.8 : 1.0\n",
      "                count(v) = 2              female : male   =      8.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(6) # likelihood ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choosing the right features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features2(name):\n",
    "    features = {}\n",
    "    features['first_letter'] = name[0].lower()\n",
    "    features['last_letter'] = name[-1].lower()\n",
    "    for x in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features['count({})'.format(x)] = name.lower().count(x)\n",
    "        features['has({})'.format(x)] = (x in name.lower())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#gender_features2('John')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_names = labeled_names[1500:]\n",
    "devtest_names = labeled_names[500:1500]\n",
    "test_names = labeled_names[:500]\n",
    "#len(train_names), len(devtest_names), len(test_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply features to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = apply_features(gender_features, train_names)\n",
    "devtest_set = apply_features(gender_features, devtest_names)\n",
    "test_set = apply_features(gender_features, test_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.786\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, devtest_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct=female  guess=male    name=Addis                         \n",
      "correct=female  guess=male    name=Alis                          \n",
      "correct=female  guess=male    name=Allis                         \n",
      "correct=female  guess=male    name=Anais                         \n",
      "correct=female  guess=male    name=Annabel                       \n",
      "correct=female  guess=male    name=Annabell                      \n",
      "correct=female  guess=male    name=Aryn                          \n",
      "correct=female  guess=male    name=Beret                         \n",
      "correct=female  guess=male    name=Bliss                         \n",
      "correct=female  guess=male    name=Brigit                        \n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "\n",
    "for x, y in devtest_names:\n",
    "    guess = classifier.classify(gender_features(x))\n",
    "    if guess != y:\n",
    "        errors.append((y, guess, x))\n",
    "        \n",
    "for (y, guess, x) in sorted(errors)[:10]:\n",
    "    print('correct={:<8}' 'guess={:<8s}' 'name={:<30}'.format(y, guess, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'suffix1': 'n', 'suffix2': 'yn'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gender_features(name):\n",
    "    return {'suffix1': name[-1],\n",
    "            'suffix2': name[-2:]}\n",
    "\n",
    "gender_features('Ailyn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.802\n"
     ]
    }
   ],
   "source": [
    "train_set = apply_features(gender_features, train_names)\n",
    "devtest_set = apply_features(gender_features, devtest_names)\n",
    "test_set = apply_features(gender_features, test_names)\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, devtest_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                 suffix2 = 'na'           female : male   =     85.0 : 1.0\n",
      "                 suffix2 = 'la'           female : male   =     67.3 : 1.0\n",
      "                 suffix2 = 'ra'           female : male   =     53.4 : 1.0\n",
      "                 suffix2 = 'ia'           female : male   =     48.7 : 1.0\n",
      "                 suffix1 = 'a'            female : male   =     36.1 : 1.0\n",
      "                 suffix1 = 'k'              male : female =     27.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final accuracy test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.778\n",
      "Neo is a female\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "name = 'Neo'\n",
    "print('{} is a '.format(name) + classifier.classify(gender_features2(name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree entropy and information gain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def entropy(labels):\n",
    "    freqdist = nltk.FreqDist(labels)\n",
    "    probs = [freqdist.freq(x) for x in freqdist]\n",
    "    return -sum(x * math.log(x,2) for x in probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('male', 3), ('female', 1)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels1 = ['female', 'male', 'male', 'male']\n",
    "freqdist = nltk.FreqDist(labels1)\n",
    "freqdist.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male\n",
      "female\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x in freqdist:\n",
    "    print(x)\n",
    "    \n",
    "freqdist.freq('male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8112781244591328"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(labels1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Based on word frequency (Naive Bayes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "documents = [(list(movie_reviews.words(y)), x)\n",
    "            for x in movie_reviews.categories()\n",
    "            for y in movie_reviews.fileids(x)]\n",
    "\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(x.lower() for x in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document) # checking an 'IF x in y' statement is faster in a set than in a list\n",
    "    features = {}\n",
    "    for x in word_features:\n",
    "        features['contains({})'.format(x)] = (x in document_words)\n",
    "    return features\n",
    "\n",
    "#print(document_features(movie_reviews.words('pos/cv957_8737.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 200)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_length = len(documents)\n",
    "data_length = int(len(documents) * 0.9)\n",
    "\n",
    "train_set = apply_features(document_features, documents[:data_length])\n",
    "test_set = apply_features(document_features, documents[data_length:])\n",
    "\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.835"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "   contains(outstanding) = True              pos : neg    =     10.4 : 1.0\n",
      "         contains(mulan) = True              pos : neg    =      8.3 : 1.0\n",
      "        contains(wasted) = True              neg : pos    =      7.6 : 1.0\n",
      "   contains(wonderfully) = True              pos : neg    =      7.3 : 1.0\n",
      "        contains(seagal) = True              neg : pos    =      6.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "\n",
    "for x, y in documents[int(data_length * 0.9):]:\n",
    "    guess = classifier.classify(document_features(x))\n",
    "    if guess != y:\n",
    "        errors.append((y, guess, x))\n",
    "        \n",
    "#for (y, guess, x) in sorted(errors):\n",
    "#    print('correct=: ', y, 'guess=: ', guess, 'document= ', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Based on word structure (Decision Tree):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "suffix_fdist = nltk.FreqDist()\n",
    "for x in brown.words(): # compile suffix freq dist\n",
    "    suffix_fdist[x[-1:]] += 1\n",
    "    suffix_fdist[x[-2:]] += 1\n",
    "    suffix_fdist[x[-3:]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', ',', '.', 's', 'd', 't', 'n', 'he', 'of', 'a']\n"
     ]
    }
   ],
   "source": [
    "common_suffixes = [x for x, y in suffix_fdist.most_common(100)]\n",
    "print(common_suffixes[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature extractor:\n",
    "\n",
    "def pos_features(word):\n",
    "    features = {}\n",
    "    for x in common_suffixes:\n",
    "        features['endswith({})'.format(x)] = word.lower().endswith(x)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature set for each data point\n",
    "\n",
    "tagged_words = brown.tagged_words(categories='news')\n",
    "#featuresets = [(pos_features(x), y) for x, y in tagged_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90499, 10055)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into training and testing sets\n",
    "\n",
    "size = int(len(tagged_words) * 0.1)\n",
    "\n",
    "#train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "train_set = apply_features(pos_features, tagged_words[size:])\n",
    "test_set = apply_features(pos_features, tagged_words[:size])\n",
    "\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train classifier\n",
    "\n",
    "classifier = nltk.DecisionTreeClassifier.train(train_set[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, test_set[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NNS'"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(pos_features('cats'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train in module nltk.classify.decisiontree:\n",
      "\n",
      "train(labeled_featuresets, entropy_cutoff=0.05, depth_cutoff=100, support_cutoff=10, binary=False, feature_values=None, verbose=False)\n",
      "    :param binary: If true, then treat all feature/value pairs as\n",
      "        individual binary features, rather than using a single n-way\n",
      "        branch for each feature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.DecisionTreeClassifier.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if endswith(he) == False: \n",
      "  if endswith(s) == False: \n",
      "    if endswith(.) == False: \n",
      "      if endswith(,) == False: return 'WDT'\n",
      "      if endswith(,) == True: return ','\n",
      "    if endswith(.) == True: return '.'\n",
      "  if endswith(s) == True: \n",
      "    if endswith(as) == False: \n",
      "      if endswith(is) == False: return 'NNS'\n",
      "      if endswith(is) == True: return 'BEZ'\n",
      "    if endswith(as) == True: \n",
      "      if endswith(was) == False: return 'HVZ'\n",
      "      if endswith(was) == True: return 'BEDZ'\n",
      "if endswith(he) == True: \n",
      "  if endswith(the) == False: return 'PPS'\n",
      "  if endswith(the) == True: return 'AT'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display tree structure\n",
    "\n",
    "print(classifier.pseudocode(depth=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Based on word structure and previous word (Naive Bayes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'suffix(1)': 'n', 'suffix(2)': 'on', 'suffix(3)': 'ion', 'prev-word': 'an'}"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define context-dependant feature extractor\n",
    "\n",
    "def pos_features_context(sentence, i):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "    return features\n",
    "               \n",
    "pos_features_context(brown.sents()[0], 8)                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build a labeled featureset for every word in every sentence\n",
    "\n",
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "#featuresets = []\n",
    "\n",
    "#for x in tagged_sents:\n",
    "#    untagged_sent = nltk.tag.untag(x)\n",
    "#    for i, (word, tag) in enumerate(x):\n",
    "#        featuresets.append( (pos_features_context(untagged_sent, i), tag) )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureset_buider(tagged_sents):\n",
    "    featuresets = []\n",
    "    for x in tagged_sents:\n",
    "        untagged_sent = nltk.tag.untag(x)\n",
    "        for i, (word, tag) in enumerate(x):\n",
    "            featuresets.append( (pos_features_context(untagged_sent, i), tag) )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "featureset_buider(tagged_sents[size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'suffix(1)': 'e',\n",
       "  'suffix(2)': 'he',\n",
       "  'suffix(3)': 'The',\n",
       "  'prev-word': '<START>'},\n",
       " 'AT')"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90499, 10055)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare train and test sets\n",
    "\n",
    "size =  int(len(featuresets) * 0.1)\n",
    "\n",
    "#train_set = apply_features(featureset_buider, tagged_sents[size:])\n",
    "#test_set = apply_features(featureset_buider, tagged_sents[:size])\n",
    "train_set, test_set = featuresets[size:], featuresets[:size] # rebuild using apply_features() function\n",
    "\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7891596220785678"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and test\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               suffix(1) = '.'                 . : NN     =   6950.8 : 1.0\n",
      "               suffix(2) = 'he'               AT : NN     =   3296.2 : 1.0\n",
      "               suffix(2) = 'ho'              WPS : NN     =   2982.4 : 1.0\n",
      "               suffix(1) = 'r'               JJR : NNS    =   2252.6 : 1.0\n",
      "               suffix(2) = 'to'               TO : JJ     =   2180.6 : 1.0\n",
      "               suffix(1) = 'h'               ABX : NNS    =   2013.7 : 1.0\n",
      "               suffix(2) = 'es'              NNS : IN     =   1676.3 : 1.0\n",
      "               suffix(3) = 'hat'              CS : NN     =   1576.4 : 1.0\n",
      "               suffix(1) = \"'\"                '' : JJ     =   1502.2 : 1.0\n",
      "               suffix(2) = 'ng'              VBG : VBN    =   1241.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Based on word structure, previous word and previous word tag (Naive Bayes, joint classifier):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "\n",
    "def pos_features_prev_tag(sentence, i, history): \n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                 \"suffix(2)\": sentence[i][-2:],\n",
    "                 \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "        features[\"prev-tag\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "        features[\"prev-tag\"] = history[i-1]\n",
    "    return features\n",
    "\n",
    "class ConsecutivePosTagger(nltk.TaggerI): \n",
    "\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = pos_features_prev_tag(untagged_sent, i, history)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = pos_features_prev_tag(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7980528511821975\n"
     ]
    }
   ],
   "source": [
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "\n",
    "size =  int(len(tagged_sents) * 0.1)\n",
    "train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]  # rebuild using apply_features() function\n",
    "tagger = ConsecutivePosTagger(train_sents)\n",
    "print(tagger.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               suffix(1) = '.'                 . : NN     =   6881.7 : 1.0\n",
      "               suffix(2) = 'he'               AT : NN     =   3266.7 : 1.0\n",
      "                prev-tag = 'TO'               VB : NN     =   3227.5 : 1.0\n",
      "               suffix(2) = 'ho'              WPS : NN     =   2940.6 : 1.0\n",
      "                prev-tag = 'MD'               BE : NP     =   2253.7 : 1.0\n",
      "               suffix(1) = 'r'               JJR : NNS    =   2223.1 : 1.0\n",
      "               suffix(2) = 'to'               TO : JJ     =   2165.0 : 1.0\n",
      "               suffix(1) = 'h'               ABX : NNS    =   1954.4 : 1.0\n",
      "               suffix(2) = 'es'              NNS : IN     =   1648.4 : 1.0\n",
      "               suffix(3) = 'hat'              CS : NN     =   1528.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "tagger.classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Further examples of supervised classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence segmentation (to be continued):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = nltk.corpus.treebank_raw.sents()\n",
    "tokens = []\n",
    "boundaries = set()\n",
    "offset = 0\n",
    "for x in sents:\n",
    "    tokens.extend(x) # can add multiple individual elements to the list\n",
    "    offset += len(x)\n",
    "    boundaries.add(offset-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indentifying dialogue act types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = nltk.corpus.nps_chat.xml_posts()[:10000] # to get a data structure representing the XML annotation for each post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define feature extractor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dialogue_act_features(post):\n",
    "    features = {}\n",
    "    for x in nltk.word_tokenize(post):\n",
    "        features['contains({})'.format(x.lower())] = True\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contains(now)': True,\n",
       " 'contains(im)': True,\n",
       " 'contains(left)': True,\n",
       " 'contains(with)': True,\n",
       " 'contains(this)': True,\n",
       " 'contains(gay)': True,\n",
       " 'contains(name)': True}"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogue_act_features(posts[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'now im left with this gay name'"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['now', 'im', 'left', 'with', 'this', 'gay', 'name']"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(posts[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create featuresets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(dialogue_act_features(x.text), x.get('class')) for x in posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'contains(now)': True,\n",
       "  'contains(im)': True,\n",
       "  'contains(left)': True,\n",
       "  'contains(with)': True,\n",
       "  'contains(this)': True,\n",
       "  'contains(gay)': True,\n",
       "  'contains(name)': True},\n",
       " 'Statement')"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 1000)"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare train and test sets\n",
    "\n",
    "random.shuffle(featuresets)\n",
    "size =  int(len(featuresets) * 0.1)\n",
    "\n",
    "#train_set = apply_features(dialogue_act_features, posts[size:])\n",
    "#test_set = apply_features(dialogue_act_features, posts[:size])\n",
    "train_set, test_set = featuresets[size:], featuresets[:size] # rebuild using apply_features() function\n",
    "\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.669\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "            contains(hi) = True            Greet : System =    395.0 : 1.0\n",
      "             contains(>) = True            Other : System =    321.1 : 1.0\n",
      "           contains(brb) = True              Bye : Statem =    305.9 : 1.0\n",
      "            contains(no) = True           nAnswe : System =    304.2 : 1.0\n",
      "          contains(part) = True           System : Statem =    303.6 : 1.0\n",
      "             contains(0) = True            Other : Statem =    284.8 : 1.0\n",
      "          contains(nope) = True           nAnswe : Statem =    281.2 : 1.0\n",
      "           contains(yes) = True           yAnswe : Emotio =    246.0 : 1.0\n",
      "             contains(<) = True            Other : Greet  =    231.9 : 1.0\n",
      "           contains(are) = True           whQues : System =    192.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recognising textual entailment (RTE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rte_features(rtepair): # some high frequency function words are filtered out as 'stopwords'\n",
    "    extractor = nltk.RTEFeatureExtractor(rtepair)\n",
    "    features = {}\n",
    "    features['word_overlap'] = len(extractor.overlap('word'))\n",
    "    features['word_hyp_extra'] = len(extractor.hyp_extra('word'))\n",
    "    features['ne_overlap'] = len(extractor.overlap('ne')) # named entity\n",
    "    features['ne_hyp_extra'] = len(extractor.hyp_extra('ne'))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word_overlap': 0, 'word_hyp_extra': 1, 'ne_overlap': 1, 'ne_hyp_extra': 1}"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rte_features(rtepair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Russia', 'Davudi', 'association', 'at', 'four', 'fight', 'terrorism.', 'fledgling', 'Iran', 'Shanghai', 'Soviet', 'SCO', 'operation', 'representing', 'former', 'that', 'Parviz', 'Asia', 'central', 'together', 'republics', 'Organisation', 'was', 'binds', 'meeting', 'China', 'Co'}\n"
     ]
    }
   ],
   "source": [
    "rtepair = nltk.corpus.rte.pairs(['rte3_dev.xml'])[33]\n",
    "extractor = nltk.RTEFeatureExtractor(rtepair)\n",
    "print(extractor.text_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'member', 'SCO.', 'China'}\n"
     ]
    }
   ],
   "source": [
    "print(extractor.hyp_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set() {'China'} {'member'}\n"
     ]
    }
   ],
   "source": [
    "print(extractor.overlap('word'), extractor.overlap('ne'), extractor.hyp_extra('word')) # didn't pick up SCO due to period (SCO.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "help(nltk.RTEFeatureExtractor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
